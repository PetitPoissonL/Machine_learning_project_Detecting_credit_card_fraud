{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335bd89f",
   "metadata": {},
   "source": [
    "# Détection des fraudes à la carte de crédit #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824042e",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "### 1.1. Introduction du projet\n",
    "Ce projet consiste à construire un modèle de prédiction anti-fraude des cartes de crédit en utilisant les données historiques des transactions par carte de crédit, afin de détecter à l'avance le vol des cartes de crédit des clients.\n",
    "### 1.2. Dataset\n",
    "Nous utilisont le jeu de données sur la site https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud.\n",
    ">The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    ">\n",
    ">It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "### 1.3. Scénarios de données\n",
    "Ce jeu de données est constitué des transactions par carte de crédit, la problématique est de prédire si le client sera victime de fraude à la carte de crédit. Il y a seulement deux situations : fraude et non fraude. Et comme les données sont déjà classifiés par la colonne \"Class\", il s'agit d'un scénario d'apprentissage supervisé. C'est la raison pour laquelle la prédiction de fraude à la carte de crédit est une problématique de classification binaire. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d26712",
   "metadata": {},
   "source": [
    "## 2. Prétraitement des données ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f4218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# math\n",
    "import math\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# time\n",
    "import time\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Visualisation des données\n",
    "from plotly import subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# sampling\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "#over\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "#under\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "#mix\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096abe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc961e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9cca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f88a79",
   "metadata": {},
   "source": [
    ">Il n'y a pas de valeurs manquantes dans les données et aucun traitement des valeurs manquantes n'est nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cae47",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "Le Feature Engineering est un processus qui consiste à transformer les données brutes en caractéristiques représentant plus précisément le problème sous-jacent au modèle prédictif. https://datascientest.com/feature-engineering\n",
    "\n",
    "Comme les colonnes V1 à V28, les données sont transformés par PCA, on a pas besoin de faire la Feature Extraction. Par contre, les colonnes \"Time\" et \"Amount\" ont les types de données très différents par rapport les autres, il faut faire la **Feature scaling**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb548c",
   "metadata": {},
   "source": [
    "### 3.1. Feature Selection\n",
    "Dans le jeu de données, il y a 30 variables. Si nous utilisont tous les varisables, nous risqueront de faire le sur-apprentissage. Pour éviter cette situation, nous ferons la Feature Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a0f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformer la colonne \"Time\" en heure de la journée\n",
    "\n",
    "df['Hour'] = df[\"Time\"].apply(lambda x : divmod(x, 3600)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hour(x):\n",
    "    if x >= 24 :\n",
    "        return x -24\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour'] = df['Hour'].apply(lambda x : convert_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cb3c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34f631",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fraud = df.loc[df[\"Class\"]==1]\n",
    "df_nonfraud = df.loc[df[\"Class\"]==0]\n",
    "\n",
    "# regarder la correlation entre les variables\n",
    "# Ici, on suppose que la relation est linéaire, la valeur est dans [-1,1]\n",
    "# Si la valeur vaut 1, les variables sont en relation linéaire\n",
    "corrs_nonfraud = df_nonfraud.loc[:, df.columns != \"Class\"].corr()\n",
    "corrs_fraud = df_fraud.loc[:, df.columns != \"Class\"].corr()\n",
    "\n",
    "# affichier seulement le triangle\n",
    "mask_fraud = np.triu(np.ones_like(corrs_fraud, dtype=bool))\n",
    "corrs_fraud = corrs_fraud.mask(mask_fraud)\n",
    "mask_nonfraud = np.triu(np.ones_like(corrs_nonfraud, dtype=bool))\n",
    "corrs_nonfraud = corrs_nonfraud.mask(mask_nonfraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76301bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    corrs_nonfraud.to_numpy().round(2), \n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    x=list(corrs_nonfraud.index.values),\n",
    "    y=list(corrs_nonfraud.columns.values)\n",
    ")\n",
    "fig.update_layout(\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    title_text='Non Fraud', title_x=0.5\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    corrs_fraud.to_numpy().round(2), \n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    x=list(corrs_fraud.index.values),\n",
    "    y=list(corrs_fraud.columns.values)\n",
    ")\n",
    "fig.update_layout(\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    title_text='Fraud', title_x=0.5\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b673347",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### 3.1.1. Correlation entre les variables\n",
    "Nous recherchons la correlation entre les variables. Nous supposons que les relations suivent la loi normal.  \n",
    ">Selon les figure en dessus, dans la situation de fraud, les corrélations entre certaines variables sont plus prononcées. La variation entre V1, V2, V3, V4, V5, V6, V7, V9, V10, V11, V12, V14, V16, V17, V18 et V19 montre un certain schéma dans l'échantillon d'écrémage de cartes de crédit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3b98b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# relation entre les feutures et Class\n",
    "\n",
    "def distplot(data):\n",
    "    group_labels = ['fraud', 'non fraud']\n",
    "    hist_data = [df[data][df[\"Class\"]==1], df[data][df[\"Class\"]==0]]\n",
    "    fig = ff.create_distplot(hist_data, group_labels, bin_size=.5, show_rug=False)\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', showline=True, linecolor='LightGrey', mirror=True)\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', showline=True, linecolor='LightGrey', mirror=True)\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)', \n",
    "        height=350, \n",
    "        title=\"histogramme de \" + data,\n",
    "        title_font_color=\"Grey\",\n",
    "    )\n",
    "    fig.show()\n",
    "    return\n",
    "\n",
    "variables = df.iloc[:,1:29]\n",
    "for variable in variables:\n",
    "    distplot(variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceaf099",
   "metadata": {},
   "source": [
    "#### 3.1.2. Distribution des données\n",
    "Nous regardons à nouveau la distribution des données. Nous préférons sélectionner des variables qui ont des distributions significativement différentes sous différentes classes. Comme le montre les images ci-dessus, nous voudrions supprimer les variables V8, V13, V15, V20, V21, V22, V23, V24, V25, V26, V27 et V28. Cela est également conforme à la conclusion à laquelle nous sommes parvenus au chapitre 3.1.1. Nous supprimons aussi la colonne \"Time\" et gardons la colonne \"Hour\" en considérant le niveau de dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4486d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_drop = ['V8', 'V13', 'V15', 'V17', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Time']\n",
    "df = df.drop(list_drop, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3fb86",
   "metadata": {},
   "source": [
    "## 3.2. Feature Scaling\n",
    "Par rapport aux autres colonnes, les caractéristiques des données des colonnes \"Hour\" et \"Amount\" sont très différentes. Nous utilisons l'approche de standardisation, qui maintient l'information utile contenue dans les valeurs aberrantes et rend l'algorithme moins affecté par les valeurs aberrantes. (voir le lien https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992157e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standardisation\n",
    "\n",
    "col = ['Amount', 'Hour']\n",
    "sc = StandardScaler()\n",
    "df[col] = sc.fit_transform(df[col])\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaca04f",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle\n",
    "### 4.1. Dataset \"train\" et \"test\"\n",
    "Nous séparerons le jeu de données en  parties \"entraînement\", \"évaluation\" et \"test\" en adoptant la validation croisée pour éviter la situation sur-apprantisage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcba55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variables = list(df.columns)\n",
    "x_variables.remove('Class') #supprimer la colonne cible \"Class\" pour x\n",
    "X = df[x_variables] \n",
    "y = df[\"Class\"]\n",
    "n_echantillon = y.shape[0]\n",
    "n_echantillon_pos = y[y==0].shape[0]\n",
    "n_echantillon_neg = y[y==1].shape[0]\n",
    "print('nombre d\\'échantillons: {}; les échantillons positifs: {:.2%}; les échantillons négatifs: {:.2%}'.format(n_echantillon, n_echantillon_pos/n_echantillon, n_echantillon_neg/n_echantillon))\n",
    "print('nb de variables : ', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparer le jeu de données en partie train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données jusqu'à 2 dimensions en utilisant PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca = pd.DataFrame(X_pca)\n",
    "X_pca.columns=[\"pca_a\",\"pca_b\"]\n",
    "X_pca[\"y\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050231b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# affichage de la distribution de données\n",
    "\n",
    "sns.set()\n",
    "sns.lmplot(x=\"pca_a\", y=\"pca_b\",data=X_pca, hue=\"y\", fit_reg=False, markers=[\"o\",\"x\"],height=8,aspect=1.5,legend=False)\n",
    "plt.legend(fontsize=20,bbox_to_anchor=(0.98, 0.6),edgecolor ='r')   \n",
    "plt.xlabel(\"axis_1\",fontsize=17)\n",
    "plt.ylabel(\"axis_2\",fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d090af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_pca = pd.DataFrame(X_pca)\n",
    "X_pca.columns=[\"pca_a\",\"pca_b\"]\n",
    "X_pca[\"y\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69705a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = RandomUnderSampler()\n",
    "xx, yy = rand.fit_resample(X_train, y_train)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(xx)\n",
    "X_pca = pd.DataFrame(X_pca)\n",
    "X_pca.columns=[\"pca_a\",\"pca_b\"]\n",
    "X_pca[\"y\"] = yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ea500",
   "metadata": {},
   "source": [
    "### 4.2. Régression logistique\n",
    "#### 4.2.1. Traitement le déséquilibre de données\n",
    "Pour la variable cible \"Class\", les valeurs sont très déséquilibrées, cela peut avoir un impact sur l'apprentissage des modèles. Alors nous allons tester les méthodes sampling afin de traiter le déséquilibre de l'échantillon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choisir les méthodes sampling\n",
    "sampling_methods = [\n",
    "    SMOTE(random_state=42), \n",
    "    BorderlineSMOTE(random_state=42, kind='borderline-1'), \n",
    "    ADASYN(random_state=42), \n",
    "    NearMiss(),\n",
    "    RandomUnderSampler(),\n",
    "    SMOTEENN(random_state=42, n_jobs=-1), \n",
    "    SMOTETomek(random_state=42, n_jobs=-1)\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'SMOTE', \n",
    "    'Borderline SMOTE', \n",
    "    'ADASYN', \n",
    "    'NearMiss',\n",
    "    'RandomUnderSampler',\n",
    "    'SMOTE+ENN', \n",
    "    'SMOTE+Tomek'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30d24b",
   "metadata": {},
   "source": [
    "#### 4.2.2. Entraînement le modèle\n",
    "Nous séparerons le jeu de données en 3 parts \"entraînement\", \"évaluation\" et \"test\" en adoptant la validation croisée pour éviter la situation sur-apprantisage. Le modèle peut apprendre sur l'ensemble d'entraînement, les paramètres sont réglés sur l'ensemble d'évaluation et, enfin, la performance du modèle est évaluée à l'aide des données de l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_method(method):\n",
    "    count = 0\n",
    "    xx, yy = method.fit_resample(X_train, y_train)\n",
    "    y_pred, y_prob = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "    for X_ensemble, y_ensemble in zip(xx, yy):\n",
    "        model = LogisticRegression()  \n",
    "        model.fit(X_ensemble, y_ensemble)\n",
    "        y_pred += model.predict(X_test)\n",
    "        y_prob += model.predict_proba(X_test)[:, 1]\n",
    "        count += 1\n",
    "    return np.where(y_pred >= 0, 1, -1), y_prob/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9835dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# courbe ROC\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "for (name, method) in zip(names, sampling_methods):\n",
    "    t0 = time.time()\n",
    "    model = make_pipeline(method, LogisticRegression())  \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label=1)\n",
    "    plt.plot(fpr, tpr, lw=3, label='{} (AUC={:.2f}, time={:.2f}s)'.\n",
    "             format(name, auc(fpr, tpr), time.time() - t0))\n",
    "    plt.xlabel(\"FPR\", fontsize=17)\n",
    "    plt.ylabel(\"TPR\", fontsize=17)\n",
    "    plt.title(\"ROC Curve\",fontsize=17)\n",
    "    plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd299f36",
   "metadata": {},
   "source": [
    ">Selon le résultat, les courbes ROC présentent une estimation excessivement optimiste de l'effet sauf la méthode NearMiss, du coup nous voudrions utiliser la coubre PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e89a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# courbe PR (Precision Recall) \n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "for (name, method) in zip(names, sampling_methods):\n",
    "    t0 = time.time()\n",
    "    model = make_pipeline(method, LogisticRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_prob, pos_label=1)\n",
    "    plt.plot(recall, precision, lw=3, label='{} (AUC={:.2f}, time={:.2f}s)'.\n",
    "             format(name, auc(recall, precision), time.time() - t0))\n",
    "    plt.xlabel(\"Recall\", fontsize=17)\n",
    "    plt.ylabel(\"Precision\", fontsize=17)\n",
    "    plt.title(\"PR Curve\",fontsize=17)\n",
    "    plt.legend(fontsize=14, loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd94644",
   "metadata": {},
   "source": [
    ">Selon la courbe PR, nous préférons choisir la méthode SMOTE+ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['non fraud', 'fraud']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"------------------------Original---------------------- \\n\", \n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')\n",
    "\n",
    "model = make_pipeline(SMOTE(random_state=42), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"--------------------------SMOTE----------------------- \\n\",\n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')\n",
    "\n",
    "model = make_pipeline(RandomUnderSampler(), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"-------------------RandomUnderSampler----------------- \\n\",\n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')\n",
    "\n",
    "model = make_pipeline(SMOTEENN(random_state=42, n_jobs=-1), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"------------------------SMOTEENN---------------------- \\n\",\n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c6c71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# changer le seuil\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95, random_state=0, stratify=y)\n",
    "\n",
    "\n",
    "class_names = ['non fraud', 'fraud']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"------------------------Original---------------------- \\n\", \n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')\n",
    "\n",
    "model = make_pipeline(SMOTE(random_state=42), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"--------------------------SMOTE----------------------- \\n\",\n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')\n",
    "\n",
    "model = make_pipeline(SMOTEENN(random_state=42, n_jobs=-1), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"------------------------SMOTEENN---------------------- \\n\",\n",
    "      classification_report(y_test, y_pred, target_names=class_names), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5734a",
   "metadata": {},
   "source": [
    "#### 4.2.2 Optimisation du modèle\n",
    "Nous trouverons les meilleurs paramètres dans LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bae677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation avec GridSearchCV\n",
    "\n",
    "model = Pipeline([\n",
    "    ('sampling', SMOTE(random_state=42)),\n",
    "    ('classification', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classification__C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, params, n_jobs=-1, scoring='f1', cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "results = pd.DataFrame(grid.cv_results_) \n",
    "best = np.argmax(results.mean_test_score.values)\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.5f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation avec KFold\n",
    "\n",
    "list_C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "def find_best_C(x_train, y_train):\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    kf = KFold(n_splits=10)\n",
    "    best_C = 0\n",
    "    best_f1 = 0\n",
    "    for C in list_C:\n",
    "        sum_f1 = 0\n",
    "        nb_test = 0\n",
    "        for i_train, i_test in kf.split(x_train):\n",
    "            x_train_kf, x_test_kf = x_train[i_train], x_train[i_test]\n",
    "            y_train_kf, y_test_kf = y_train[i_train], y_train[i_test]\n",
    "            model = make_pipeline(SMOTE(random_state=42), LogisticRegression(C=C))\n",
    "            model.fit(x_train_kf, y_train_kf)\n",
    "            pred_y = model.predict(x_test_kf)\n",
    "            sum_f1 += f1_score(y_test_kf, pred_y)\n",
    "            nb_test += 1\n",
    "        aveg_f1 = sum_f1/nb_test\n",
    "        if aveg_f1 > best_f1:\n",
    "            best_C = C\n",
    "            best_f1 = aveg_f1\n",
    "    return (best_C, best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_C, best_f1) = find_best_C(X_train, y_train)\n",
    "print('Best C : ', best_C)\n",
    "print('Best f1 : ', best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(x_train, y_train)\n",
    "prediction_test = clf.predict(x_test)\n",
    "print(\"accuracy_test : {:.10f}\".format(accuracy_score(prediction_test, y_test, normalize=True)))\n",
    "print(\"recall_test : {:.10f}\".format(recall_score(prediction_test, y_test)))\n",
    "print(\"precision_test : {:.10f}\".format(precision_score(prediction_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
